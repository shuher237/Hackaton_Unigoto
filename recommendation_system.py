# -*- coding: utf-8 -*-
"""Recommendation system.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8-tKhaegidmGAUu9HcyUUMDikT8QHKA

#Read data from processed dataframe
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
home_dir = "/content/drive/My Drive/Hackaton UNIGOTO 28.11/"

df_copy = pd.read_csv(home_dir+'data_cleaned4_combined.csv')

"""#Imitation user input"""

# @title
top_n = 10 # @param {type:"slider", min:0, max:20, step:1}
import ipywidgets as widgets
from IPython.display import display
import numpy as np

city = ''
education_form = ''
education_status = ''

random_numbers = [''] + df_copy['city'].value_counts().index.values.tolist()
Dropdown = widgets.Dropdown(
options=random_numbers,
description='Please, choose a city from the list',
  )
output = widgets.Output()

def onchange(change):
  global city
  city = change['new']

Dropdown.observe(onchange, names='value')
display(Dropdown)

about = str(input("Напишите о себе: "))
activities = str(input("Напишите о своих увлечениях: "))
books = str(input("Укажите ваши любимые книги: "))
games = str(input("Укажите ваши любимые игры: "))
interests = str(input("Напишите о своих интересах: "))



random_numbers = [''] + df_copy['education_form'].value_counts().index.values.tolist()
Dropdown = widgets.Dropdown(
options=random_numbers,
description='Please, choose a education_form from the list',
  )
output = widgets.Output()

def onchange(change):
  global education_form
  education_form = change['new']

Dropdown.observe(onchange, names='value')
display(Dropdown)

random_numbers = [''] + df_copy['education_status'].value_counts().index.values.tolist()
Dropdown = widgets.Dropdown(
options=random_numbers,
description='Please, choose a education_status from the list',
  )
output = widgets.Output()

def onchange(change):
  global education_status
  education_status = change['new']

Dropdown.observe(onchange, names='value')
display(Dropdown)

user_input = [city, about, activities, books, games, interests, education_form, education_status]
user_input

"""#TF-IDF recommendations"""

df_copy.fillna("", inplace=True)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

! pip install pymorphy2

import re

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords

import pymorphy2

nltk.download('punkt')
nltk.download('stopwords')

stopwords_ru = stopwords.words('russian')
morph = pymorphy2.MorphAnalyzer()

def clean_text(text: str) -> str:
    text = re.sub(r'((http|ftp)\S+)|[^a-zа-яё\s/-]|(-{2,})', '', text.lower())
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def remove_stopwords(tokens: list, stopwords=None) -> list:
    if not stopwords:
        return tokens

    stopwords = set(stopwords)
    tokens = [tok for tok in tokens if tok not in stopwords and len(tok)>2]

    return tokens

def text_preparation(text: str) -> str:
    text = clean_text(text)
    tokens = word_tokenize(text, language='russian')
    tokens = remove_stopwords(tokens, stopwords_ru)
    tokens = [morph.parse(tok)[0].normal_form for tok in tokens]

    return ' '.join(tokens)

tfidf = TfidfVectorizer(tokenizer=lambda x: x, stop_words=stopwords_ru, lowercase=False)

def get_recommendations():
    '''Функция для получения рекомендаций на основе введенных пользователем данных
    '''
    if city != '':
      df = df_copy[(df_copy['city'] == city)]
    else:
      df = df_copy
    if education_form != '':
      df = df[(df['education_form'] == education_form)]
    else:
      df = df


    #user_input = city + ' ' + about + ' ' + activities + ' ' + books + ' ' + games + ' ' + interests
    user_input = text_preparation(about) + text_preparation(activities) + text_preparation(books) + text_preparation(games) + text_preparation(interests)
    user_tfidf = tfidf.fit_transform([user_input]+df['combined'].tolist())

    # вычисление косинусного сходства между введенными данными и всеми пользователями
    cosine_sim_user = linear_kernel(user_tfidf[0], user_tfidf).flatten()
    print(cosine_sim_user.argsort(axis=0)[-1:-top_n-1:-1])
    # получение индекса пользователя с наибольшим косинусным сходством
    similar_user_indices = cosine_sim_user[1:].argsort(axis=0)[-1:-top_n-1:-1]
    # получение рекомендаций университета и факультета на основе наиболее похожего пользователя
    recommendations = df.loc[similar_user_indices, ['faculty_name', 'university_name']]

    return recommendations

get_recommendations()

"""#SentenceTransformerModel Bert (ru version)"""

! pip install sentence_transformers

import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load pre-trained BERT model
model = SentenceTransformer('DeepPavlov/rubert-base-cased')

# Compute embeddings for the introduced text
query = about + ' ' + activities + ' ' + books + ' ' + games + ' ' + interests
query = text_preparation(query)
query_embedding = model.encode(query, convert_to_tensor=True)

# Compute embeddings for the text column
text_embeddings = model.encode(df_copy['combined'].tolist(), convert_to_tensor=True)

# Compute similarity scores
cosine_scores = util.pytorch_cos_sim(query_embedding, text_embeddings)

# Add the similarity scores to the DataFrame
df_copy['similarity'] = cosine_scores.tolist()[0]

df_copy = df_copy.sort_values(by='similarity', ascending=False)

import torch

text_embeddings = torch.load(home_dir+'text_embeddings_tensor.pt')

model = SentenceTransformer('DeepPavlov/rubert-base-cased')

query = about + ' ' + activities + ' ' + books + ' ' + games + ' ' + interests

query = text_preparation(query)
query_embedding = model.encode(query, convert_to_tensor=True)
query_embedding = query_embedding.type(torch.float64)

# Compute similarity scores
cosine_scores = util.pytorch_cos_sim(query_embedding, text_embeddings)

df_copy['similarity'] = cosine_scores.tolist()[0]

df_copy = df_copy.sort_values(by='similarity', ascending=False)

if city != '':
  df = df_copy[(df_copy['city'] == city)]
else:
  df = df_copy
if education_form != '':
  df = df[(df['education_form'] == education_form)]
else:
  df = df
df[['university_name', 'faculty_name']].head(top_n)

"""#WORD2VEC recommendations"""

from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import gensim.downloader as api

# Загружаем предобученную модель Word2Vec
word2vec_model = api.load("word2vec-ruscorpora-300")

def text_to_vector(text):
    vector = np.zeros(word2vec_model.vector_size)
    words = word_tokenize(text)
    for word in words:
        if word in word2vec_model.key_to_index:
            vector += word2vec_model[word]
    return vector

user_input = text_preparation(about) + ' ' + text_preparation(activities) + ' ' + text_preparation(books) + ' ' + text_preparation(games) + ' ' + text_preparation(interests)

# Преобразуем текстовые данные в векторы Word2Vec
def get_word2vec_recommendations():
  text_vectors = df_copy['combined'].apply(lambda x: text_to_vector(x))
  text_vectors = np.stack(text_vectors.values)
  user_vectors = text_to_vector(user_input)
  user_vectors = user_vectors.reshape(1, -1)
  #user_vectors = np.stack(user_vectors.values)
  # вычисление косинусного сходства между пользователями на основе векторов Word2Vec признаков
  #cosine_sim_word2vec = linear_kernel(text_vectors, text_vectors)

  cosine_sim_user_word2vec = linear_kernel(user_vectors, text_vectors)

  # получение индекса пользователя с наибольшим косинусным сходством
  similar_user_indices = cosine_sim_user_word2vec.argsort(axis=1)[0, :-top_n-1:-1]

  # получение рекомендаций университета и факультета на основе наиболее похожего пользователя
  w2v_recommendations = df_copy.loc[similar_user_indices, ['faculty_name', 'university_name']]

  return w2v_recommendations

get_word2vec_recommendations()

"""#DOC2VEC recommendations"""

from gensim.models.doc2vec import TaggedDocument, Doc2Vec

corpus = df_copy['combined'].apply(word_tokenize)
corpus = [
    TaggedDocument(words, [idx])
    for idx, words in enumerate(corpus)
]

model = Doc2Vec(corpus, min_count=0)
print(model.dv[0])

user_input = text_preparation(about) + ' ' + text_preparation(activities) + ' ' + text_preparation(books) + ' ' + text_preparation(games) + ' ' + text_preparation(interests)
#user_input = [country, city, about, activities, books, games, interests, education_status]
results = model.dv.most_similar(positive=[model.infer_vector([text_preparation(user_input)])], topn=top_n+1)
results

idxs = [idx for idx, score in results]
df_copy.loc[idxs, ['faculty_name', 'university_name']]