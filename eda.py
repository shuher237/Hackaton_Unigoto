# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NdrvvvnHaBKcbHhKwQnLM3vma0qWQu6i

# Preparation
"""

import re
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from google.colab import drive
drive.mount('/content/drive')
home_dir = "/content/drive/My Drive/Hackaton UNIGOTO 28.11/"

df = pd.read_csv(home_dir+'data.csv')

df.shape

df.drop_duplicates(inplace=True)
df.duplicated().sum()

"""# Data Description

## Information
"""

display(df.info())
display(df.head(5))
display(df.sample(5))

"""## Numerical And Categorical Features"""

def split_features(df):
    cat_cols = []
    num_cols = []

    for col_name in df.columns:
        if df[col_name].dtypes == object:
            cat_cols.append(col_name)
        else:
            num_cols.append(col_name)

    return num_cols, cat_cols

num_cols, cat_cols = split_features(df)

print(num_cols)
print(cat_cols)

"""## Basic Statistics"""

display(df[num_cols].describe().transpose())
display(df[cat_cols].describe().transpose())

"""## Correlation"""

def display_stylized_df(df: pd.DataFrame, transposed=False, cmap='Greens'):
    if transposed:
        display(df.transpose().style.background_gradient(cmap=cmap, axis=1))
    else:
        display(df.style.background_gradient(cmap=cmap))

cmap = sns.color_palette("Greens", as_cmap=True)

display_stylized_df(df[num_cols].corr(), cmap=cmap)

"""## Missing Data"""

def get_nulls_by_cols(df: pd.DataFrame):
    nan_vals = df.isna().sum().to_frame().rename(columns={0: 'missing'})
    nan_vals = nan_vals[nan_vals['missing'] != 0].sort_values(
        'missing', ascending=False)
    nan_vals['%'] = (nan_vals['missing']/df.shape[0]*100).round(2)

    return nan_vals

def show_plots_of_nulls(df, nan_vals):
    fig, axes = plt.subplots(ncols=2, figsize=(12, 4))
    fig.suptitle('Missing Values By Columns')

    barplot = sns.barplot(
        data=nan_vals,
        x=nan_vals.index,
        y='missing',
        palette='Greens_r',
        ax=axes[0]
    )
    barplot.tick_params(axis='x', rotation=45)

    heatmap = sns.heatmap(
        df[nan_vals.index].isna(),
        cmap='Greens',
        ax=axes[1]
    )
    heatmap.tick_params(axis='x', rotation=45)

nan_vals = get_nulls_by_cols(df)
display_stylized_df(nan_vals, True)
show_plots_of_nulls(df, nan_vals)

"""## Values Analysis

### Numerical
"""

def show_boxplots(df, num_cols, width, figsize):
    height = int(np.ceil(len(num_cols)/width))
    fig, axes = plt.subplots(nrows=height, ncols=width, figsize=figsize)

    for idx, col_name in enumerate(num_cols):
        plt.subplot(height, width, idx+1)
        sns.boxplot(data=df, x=col_name, orient='h', palette='Greens')

show_boxplots(df, num_cols, 3, (15, 5))

"""### Categorical"""

df_cities_counts = df.groupby(['country', 'city'], as_index=False).size().sort_values('size', ascending=False).iloc[:20, :]
df_cities_counts

df_faculties_counts = df.groupby(['university_name', 'faculty_name'], as_index=False).size().sort_values('size', ascending=False).iloc[:20, :]
df_faculties_counts

def plot_treemap(df: pd.DataFrame, cols: list, values: str):
    fig = px.treemap(
        data_frame=df,
        values=values,
        path=cols,
        height=1000,
        width=1500
    )

    fig.show()

plot_treemap(df_faculties_counts, ['university_name', 'faculty_name'], 'size')

df['education_status'].value_counts()

df['deactivated'].value_counts()

"""# Data Manipulation"""

df_copy = df.copy()

"""## Removing Deleted And Deactivated Accounts"""

mask_deactivated = (df_copy['deactivated'] == 'banned') | (df_copy['deactivated'] == 'deleted')
df_copy = df_copy[~mask_deactivated]
df_copy.reset_index(drop=True, inplace=True)
df_copy

df_copy.drop(columns=['deactivated', 'faculty', 'university'], inplace=True)
df_copy

"""## Removing Half Empty Rows"""

cols_to_check = int(0.5*df_copy.shape[1])
print(cols_to_check, df_copy.shape[1])

df_copy.dropna(thresh=cols_to_check, inplace=True)
df_copy = df_copy.reset_index(drop=True)
df_copy

"""## Filtering Universities From Another Dataframe"""

df_universities = pd.read_excel(f'{home_dir}rf_universities.xlsx', sheet_name='Лист2', usecols=[0, 1, 3])

df_universities = df_universities.reset_index(drop=True)
df_universities = df_universities.rename(columns={'name_university': 'name', 'Повторы Номера ячеек, ссылки, удобный формат):': 'old_names_ids'})
df_universities

df_universities['same_universities_idx'] = df_universities.index.values

def fill_same_universities_id(row):
    if not pd.isnull(row['old_names_ids']) and row.name == row['same_universities_idx']:
        old_names_ids = row['old_names_ids']

        if type(old_names_ids) == int or type(old_names_ids) == float:
            old_ids = [old_names_ids]
        else:
            old_names_ids = old_names_ids.replace(',', ' ').replace('.', ' ')
            old_ids = list(map(int, old_names_ids.split()))

        df_universities.loc[df_universities['id'].isin(old_ids), 'same_universities_idx'] = row.name

    return row

df_universities.apply(fill_same_universities_id, axis=1)

df_universities['same_universities_idx'].value_counts().sort_index()

df_copy = df_copy.merge(
    df_universities[['name', 'same_universities_idx']].rename(columns={'name': 'university_name'}),
    how='inner',
    on='university_name'
)

df_copy

df_copy.to_csv(home_dir+'data_cleaned1_filter_universities.csv', index=False)

"""## Extracting Countries And Cities"""

df_copy = pd.read_csv(home_dir+'data_cleaned1_filter_universities.csv')

df_copy['country'].apply(lambda x: x.count('title') > 1).sum()

df_copy['city'].apply(lambda x: x.count('title') > 1).sum()

def extract_countries(x: str) -> list:
    countries = []

    first_idx = next(re.finditer("'title'", x)).end()
    last_idx = x.find('}', first_idx)

    return x[first_idx:last_idx].replace(":", "").replace("'", "").strip()

df_copy['country'] = df_copy.loc[:, 'country'].apply(extract_countries)

df_copy['country'].value_counts(normalize=True)

df_copy['city'] = df_copy.loc[:, 'city'].apply(extract_countries)

df_copy['city'].value_counts(normalize=True)

df_copy.to_csv(home_dir+'data_cleaned2_country_n_city_extraction.csv', index=False)

"""## Filling In Missing Data"""

df_copy = pd.read_csv(home_dir+'data_cleaned2_country_n_city_extraction.csv')

def replace_values_depending_on_frequency(df: pd.DataFrame, col: str):
    value_counts = df[col].value_counts()
    nan_count = df[col].isnull().sum()
    nan_indices = df[df[col].isnull()].index
    replace_values = np.random.choice(value_counts.index, size=nan_count, p=value_counts/value_counts.sum())
    df.loc[nan_indices, col] = replace_values

cols = [
    'country',
    'city',
    'education_form',
    'education_status',
    'university_name',
    'faculty_name',
]

for col in cols:
    replace_values_depending_on_frequency (df_copy, col)

df_copy

nan_vals = get_nulls_by_cols(df_copy)
display_stylized_df(nan_vals, True)
show_plots_of_nulls(df_copy, nan_vals)

"""## Unification Of Education Status"""

df_copy['education_status'].value_counts(dropna=False).sort_index()

def unify_education_status(x):
    if 'Абитуриентка' in x:
        return 'Абитуриент'
    elif 'Аспирантка' in x:
        return 'Аспирант'
    elif 'Выпускница' in x:
        return 'Выпускник'
    elif 'Студентка' in x:
        return 'Студент'
    elif 'Соискательница' in x:
        return 'Соискатель'
    else:
        return x

df_copy['education_status'] = df_copy['education_status'].apply(unify_education_status)
df_copy['education_status'].value_counts(dropna=False).sort_index()

df_copy.to_csv(home_dir+'data_cleaned3_unify_education_status.csv', index=False)

"""## Tokenization And Lemmatization"""

df_copy = pd.read_csv(home_dir+'data_cleaned3_unify_education_status.csv')

! pip install pymorphy2

import re

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords

import pymorphy2

nltk.download('punkt')
nltk.download('stopwords')

stopwords_ru = stopwords.words('russian')
morph = pymorphy2.MorphAnalyzer()

def clean_text(text: str) -> str:
    text = re.sub(r'((http|ftp)\S+)|[^a-zа-яё\s/-]|(-{2,})', '', text.lower())
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def remove_stopwords(tokens: list, stopwords=None) -> list:
    if not stopwords:
        return tokens

    stopwords = set(stopwords)
    tokens = [tok for tok in tokens if tok not in stopwords and len(tok)>2]

    return tokens

def text_preparation(text: str) -> str:
    text = clean_text(text)
    tokens = word_tokenize(text, language='russian')
    tokens = remove_stopwords(tokens, stopwords_ru)
    tokens = [morph.parse(tok)[0].normal_form for tok in tokens]

    return ' '.join(tokens)

def remove_special_chars(s):
    return s.replace('\r', '').replace('\n', '')

df_copy['faculty_name'] = df_copy['faculty_name'].apply(
    lambda x: x.replace('\r', '').replace('\n', '')
)

df_copy['combined'] = ['' for i in range(df_copy.shape[0])]
cols = [
    'about',
    'activities',
    'books',
    'games',
    'interests',
]

for col in cols:
    df_copy['combined'] += df_copy[col].fillna('').apply(text_preparation) + ' '

df_copy['combined'] = df_copy['combined'].apply(clean_text)

df_copy.sample(20)

df_copy.to_csv(home_dir+'data_cleaned4_combined.csv', index=False)